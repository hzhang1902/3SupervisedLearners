3900/3900 [==============================] - 0s 64us/step - loss: 2.4132 - acc: 0.2331 - val_loss: 2.1547 - val_acc: 0.3046
Epoch 2/50
3900/3900 [==============================] - 0s 22us/step - loss: 1.8843 - acc: 0.3513 - val_loss: 1.7612 - val_acc: 0.3836
Epoch 3/50
3900/3900 [==============================] - 0s 21us/step - loss: 1.6431 - acc: 0.4259 - val_loss: 1.6097 - val_acc: 0.4369
Epoch 4/50
3900/3900 [==============================] - 0s 22us/step - loss: 1.4571 - acc: 0.4921 - val_loss: 1.4092 - val_acc: 0.5251
Epoch 5/50
3900/3900 [==============================] - 0s 22us/step - loss: 1.3150 - acc: 0.5679 - val_loss: 1.2897 - val_acc: 0.5795
Epoch 6/50
3900/3900 [==============================] - 0s 20us/step - loss: 1.2241 - acc: 0.5964 - val_loss: 1.2241 - val_acc: 0.5969
Epoch 7/50
3900/3900 [==============================] - 0s 20us/step - loss: 1.1375 - acc: 0.6290 - val_loss: 1.1413 - val_acc: 0.6215
Epoch 8/50
3900/3900 [==============================] - 0s 20us/step - loss: 1.0655 - acc: 0.6615 - val_loss: 1.1186 - val_acc: 0.6318
Epoch 9/50
3900/3900 [==============================] - 0s 19us/step - loss: 1.0114 - acc: 0.6767 - val_loss: 1.0377 - val_acc: 0.6626
Epoch 10/50
3900/3900 [==============================] - 0s 17us/step - loss: 0.9740 - acc: 0.6954 - val_loss: 1.0343 - val_acc: 0.6513
Epoch 11/50
3900/3900 [==============================] - 0s 20us/step - loss: 0.9467 - acc: 0.7085 - val_loss: 0.9981 - val_acc: 0.6851
Epoch 12/50
3900/3900 [==============================] - 0s 18us/step - loss: 0.8954 - acc: 0.7236 - val_loss: 0.9436 - val_acc: 0.6964
Epoch 13/50
3900/3900 [==============================] - 0s 18us/step - loss: 0.8750 - acc: 0.7313 - val_loss: 0.9363 - val_acc: 0.6944
Epoch 14/50
3900/3900 [==============================] - 0s 30us/step - loss: 0.8488 - acc: 0.7408 - val_loss: 0.9106 - val_acc: 0.7159
Epoch 15/50
3900/3900 [==============================] - 0s 19us/step - loss: 0.8093 - acc: 0.7510 - val_loss: 0.8370 - val_acc: 0.7385
Epoch 16/50
3900/3900 [==============================] - 0s 21us/step - loss: 0.7804 - acc: 0.7641 - val_loss: 0.8100 - val_acc: 0.7497
Epoch 17/50
3900/3900 [==============================] - 0s 20us/step - loss: 0.7617 - acc: 0.7736 - val_loss: 0.8076 - val_acc: 0.7549
Epoch 18/50
3900/3900 [==============================] - 0s 21us/step - loss: 0.7338 - acc: 0.7823 - val_loss: 0.7916 - val_acc: 0.7723
Epoch 19/50
3900/3900 [==============================] - 0s 19us/step - loss: 0.7122 - acc: 0.7890 - val_loss: 0.7319 - val_acc: 0.7733
Epoch 20/50
3900/3900 [==============================] - 0s 19us/step - loss: 0.6722 - acc: 0.8051 - val_loss: 0.7337 - val_acc: 0.7846
Epoch 21/50
3900/3900 [==============================] - 0s 18us/step - loss: 0.6598 - acc: 0.8110 - val_loss: 0.7274 - val_acc: 0.7949
Epoch 22/50
3900/3900 [==============================] - 0s 17us/step - loss: 0.6469 - acc: 0.8141 - val_loss: 0.7517 - val_acc: 0.7703
Epoch 23/50
3900/3900 [==============================] - 0s 18us/step - loss: 0.6254 - acc: 0.8218 - val_loss: 0.6957 - val_acc: 0.7969
Epoch 24/50
3900/3900 [==============================] - 0s 17us/step - loss: 0.6115 - acc: 0.8277 - val_loss: 0.6750 - val_acc: 0.8021
Epoch 25/50
3900/3900 [==============================] - 0s 25us/step - loss: 0.6180 - acc: 0.8264 - val_loss: 0.6779 - val_acc: 0.8031
Epoch 26/50
3900/3900 [==============================] - 0s 19us/step - loss: 0.5928 - acc: 0.8379 - val_loss: 0.6882 - val_acc: 0.8021
Epoch 27/50
3900/3900 [==============================] - 0s 21us/step - loss: 0.5978 - acc: 0.8349 - val_loss: 0.6394 - val_acc: 0.8277
Epoch 28/50
3900/3900 [==============================] - 0s 18us/step - loss: 0.5681 - acc: 0.8438 - val_loss: 0.6273 - val_acc: 0.8215
Epoch 29/50
3900/3900 [==============================] - 0s 19us/step - loss: 0.5495 - acc: 0.8523 - val_loss: 0.6356 - val_acc: 0.8267
Epoch 30/50
3900/3900 [==============================] - 0s 17us/step - loss: 0.5352 - acc: 0.8515 - val_loss: 0.6120 - val_acc: 0.8328
Epoch 31/50
3900/3900 [==============================] - 0s 16us/step - loss: 0.5305 - acc: 0.8515 - val_loss: 0.5982 - val_acc: 0.8410
Epoch 32/50
3900/3900 [==============================] - 0s 17us/step - loss: 0.5036 - acc: 0.8659 - val_loss: 0.5779 - val_acc: 0.8492
Epoch 33/50
3900/3900 [==============================] - 0s 16us/step - loss: 0.4917 - acc: 0.8687 - val_loss: 0.5634 - val_acc: 0.8503
Epoch 34/50
3900/3900 [==============================] - 0s 16us/step - loss: 0.4930 - acc: 0.8697 - val_loss: 0.6237 - val_acc: 0.8174
Epoch 35/50
3900/3900 [==============================] - 0s 16us/step - loss: 0.4863 - acc: 0.8705 - val_loss: 0.5709 - val_acc: 0.8431
Epoch 36/50
3900/3900 [==============================] - 0s 18us/step - loss: 0.4662 - acc: 0.8736 - val_loss: 0.5781 - val_acc: 0.8441
Epoch 37/50
3900/3900 [==============================] - 0s 18us/step - loss: 0.4601 - acc: 0.8751 - val_loss: 0.5560 - val_acc: 0.8462
Epoch 38/50
3900/3900 [==============================] - 0s 20us/step - loss: 0.4471 - acc: 0.8813 - val_loss: 0.5552 - val_acc: 0.8472
Epoch 39/50
3900/3900 [==============================] - 0s 20us/step - loss: 0.4513 - acc: 0.8782 - val_loss: 0.5437 - val_acc: 0.8523
Epoch 40/50
3900/3900 [==============================] - 0s 19us/step - loss: 0.4291 - acc: 0.8869 - val_loss: 0.5346 - val_acc: 0.8574
Epoch 41/50
3900/3900 [==============================] - 0s 17us/step - loss: 0.4359 - acc: 0.8882 - val_loss: 0.5145 - val_acc: 0.8574
Epoch 42/50
3900/3900 [==============================] - 0s 18us/step - loss: 0.4308 - acc: 0.8877 - val_loss: 0.5199 - val_acc: 0.8544
Epoch 43/50
3900/3900 [==============================] - 0s 18us/step - loss: 0.4203 - acc: 0.8885 - val_loss: 0.5155 - val_acc: 0.8533
Epoch 44/50
3900/3900 [==============================] - 0s 20us/step - loss: 0.4078 - acc: 0.8931 - val_loss: 0.5500 - val_acc: 0.8492
Epoch 45/50
3900/3900 [==============================] - 0s 18us/step - loss: 0.4047 - acc: 0.8959 - val_loss: 0.5112 - val_acc: 0.8533
Epoch 46/50
3900/3900 [==============================] - 0s 16us/step - loss: 0.3961 - acc: 0.8954 - val_loss: 0.5025 - val_acc: 0.8697
Epoch 47/50
3900/3900 [==============================] - 0s 16us/step - loss: 0.4010 - acc: 0.8910 - val_loss: 0.5021 - val_acc: 0.8615
Epoch 48/50
3900/3900 [==============================] - 0s 20us/step - loss: 0.3855 - acc: 0.9013 - val_loss: 0.5043 - val_acc: 0.8585
Epoch 49/50
3900/3900 [==============================] - 0s 18us/step - loss: 0.3779 - acc: 0.9044 - val_loss: 0.4873 - val_acc: 0.8615
Epoch 50/50
3900/3900 [==============================] - 0s 17us/step - loss: 0.3692 - acc: 0.9038 - val_loss: 0.4933 - val_acc: 0.8554
{'loss': [2.4132375335693359, 1.8843349826030242, 1.6431479589755718, 1.4571438837051391, 1.3149709963187193, 1.224137617502457, 1.1374788717123179, 1.0654515493833101, 1.0113933727068778, 0.9740085403124491, 0.94666004890050648, 0.89542123721196099, 0.87501898502692199, 0.84879288722307256, 0.80933440563006276, 0.78036618244953648, 0.76171500016481453, 0.73378330090107058, 0.71217353802460892, 0.67215308201618684, 0.65978632474556942, 0.64694208304087319, 0.62541191730743795, 0.61153602979122068, 0.61804113981051323, 0.59276383748421302, 0.59783193294818582, 0.56805999596913659, 0.54952005954889149, 0.53523457435461186, 0.53051061232884722, 0.50357182142062062, 0.49166897385548325, 0.49303421436212003, 0.48632303549693179, 0.46619638754771309, 0.46010002371592401, 0.44714157031132623, 0.45127575134619691, 0.4291332933230278, 0.43586584916481608, 0.43079183719097042, 0.42032212043419864, 0.40779072303038377, 0.40465786010791094, 0.39606233144417785, 0.40101713865231248, 0.38549416841604772, 0.37789439589549334, 0.36918624376639342], 'acc': [0.23307692252672635, 0.35128205039562321, 0.42589743531667268, 0.49205128119542046, 0.56794871819324988, 0.59641025842764439, 0.62897435695697101, 0.66153846043806808, 0.67666666458814573, 0.69538461330609447, 0.70846153723887906, 0.72358974407880738, 0.73128205177111505, 0.74076923076923074, 0.75102564102564107, 0.76410256495842566, 0.77358974138895675, 0.78230769444734627, 0.78897435848529518, 0.80512820635086446, 0.81102564139243882, 0.81410256483615973, 0.82179487258960038, 0.82769230848703634, 0.82641025720498495, 0.83794871690945749, 0.83487179603332129, 0.84384615409068575, 0.85230769169636267, 0.85153846306678571, 0.85153846263885502, 0.86589743443024469, 0.86871794676169367, 0.86974358821526554, 0.87051282161321397, 0.87358974426220626, 0.87512820665652935, 0.88128205366623702, 0.87820512887759083, 0.88692307667854509, 0.88820512710473476, 0.88769230830363743, 0.88846153913400117, 0.89307692240446046, 0.89589743430797875, 0.89538461691293958, 0.89102564206490154, 0.90128205018165786, 0.90435897368651175, 0.9038461558024089], 'val_loss': [2.1547351712446945, 1.7612461900711061, 1.6096614101605538, 1.4092400320982321, 1.2896609323452681, 1.2241403249593881, 1.1412777475210336, 1.1186414417853723, 1.0376578812721449, 1.0342531829002577, 0.99814274066533792, 0.94356661117993867, 0.93629432366444509, 0.91060195702772873, 0.83701072986309344, 0.8100382060271043, 0.80763751464012346, 0.79162803399257176, 0.73189293616857287, 0.73372310467255419, 0.72735741651975194, 0.75173277292496121, 0.69572116784560378, 0.67502193567080371, 0.67785198346162456, 0.68822537085948843, 0.63937665505286978, 0.62734699634405278, 0.63563966115315751, 0.61200939465791748, 0.59824435215729932, 0.57790397087732948, 0.56343966416823554, 0.62374718433771381, 0.57088530442653562, 0.57806818858171116, 0.5559751283205473, 0.5552396141565763, 0.54371035539186918, 0.53459652063174123, 0.51452479698719122, 0.51989526473558867, 0.51549388766288762, 0.55003416886696443, 0.51119956007370582, 0.50252502557558887, 0.50207943280537926, 0.50430089904711795, 0.48729739824930829, 0.49332288100169253], 'val_acc': [0.30461538284252854, 0.38358974016629732, 0.4369230744166252, 0.52512821656007036, 0.57948718896278972, 0.59692308321977272, 0.62153846178299343, 0.63179488188181165, 0.66256410659887854, 0.65128204926466327, 0.6851282002375676, 0.69641025500419818, 0.69435896341617287, 0.71589742238704979, 0.73846153766680989, 0.74974358576994671, 0.7548718004348951, 0.77230767940863587, 0.77333334268667764, 0.78461539396872892, 0.79487178863623198, 0.77025641325192573, 0.79692307704534282, 0.8020512729424697, 0.8030769107891963, 0.80205128217354804, 0.82769230494132406, 0.82153846196639235, 0.82666667981025499, 0.83282051006952917, 0.84102563858032231, 0.84923076391220098, 0.85025639858001323, 0.81743589468491384, 0.84307691457944034, 0.84410255242616705, 0.8461538596030993, 0.84717949744982601, 0.85230769016803842, 0.85743590452732188, 0.85743590770623623, 0.85435898145039879, 0.85333334360367208, 0.84923077632219368, 0.85333334360367208, 0.86974357837285754, 0.86153846893555075, 0.85846153314297013, 0.86153846257772204, 0.85538462565495421]}
975/975 [==============================] - 0s 8us/step


************************************************************************
[===============Result=================]
Accuracy of the model is: 85.54%
[======================================]

[============Confusion Matrix===========]
Prediction --------> Row
True --------> Column

[[140   0   0   0   0   3   1   0   1   0]
 [  0 182   2   1   0   0   0   1   0   0]
 [  3   2 126   3   3   0   6   2   7   0]
 [  2   1   3  96   0   8   1   2  10   0]
 [  0   1   1   0 145   0   2   1   0   9]
 [  3   0   0   4   2  95   3   0   0   0]
 [  2   1   1   0   4   3 156   0   0   1]
 [  1   0   0   0   2   0   0 148   0   5]
 [  0   0   2   5   0   4   3   0 112   3]
 [  1   0   1   0  11   0   0   8   0 123]]
[=======================================]
************************************************************************